## Observations

1. The agent successfully learned an optimal action before the environment change, as shown by increasing cumulative reward and dominant action selection.

2. When the reward structure changed at step 500, performance temporarily declined, indicating reliance on outdated experience.

3. Continued exploration allowed the agent to adapt, leading to a shift in dominant action selection and recovery of average reward.

## Interpretation

These results demonstrate how reinforcement learning agents adapt to non-stationary environments through feedback-driven learning. The temporary performance drop highlights the exploration exploitation trade-off, while recovery shows successful adaptation.
